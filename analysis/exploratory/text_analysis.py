#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬åˆ†æ - å…³é”®è¯æå–å’Œä¸»é¢˜åˆ†æ
åˆ†æè®¨è®ºå†…å®¹ä¸­çš„å…³é”®ä¸»é¢˜å’Œæƒ…æ„Ÿå€¾å‘
"""

import json
import sys
from pathlib import Path
from collections import Counter, defaultdict
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))


def load_all_posts():
    """åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„å¸–å­æ•°æ®"""
    data_dir = project_root / "data" / "raw"

    valid_files = [
        "posts_20251121_093153.json",
        "posts_20251121_091738.json",
        "merged_posts_20251121_133607.json",
        "reddit_post_2.json",
        "reddit_post_6.json",
        "reddit_post_7.json",
        "reddit_post_10.json"
    ]

    all_posts = []
    seen_urls = set()

    ai_keywords = [
        'chatgpt', 'gpt', 'ai', 'å¤§æ¨¡å‹', 'äººå·¥æ™ºèƒ½', 'llm',
        'ç¨‹åºå‘˜', 'it', 'å¼€å‘', 'å¤±ä¸š', 'å²—ä½', 'æŠ€èƒ½',
        'èŒä¸š', 'programmer', 'developer', 'job', 'deepseek',
        'software engineer', 'coding'
    ]

    for filename in valid_files:
        file_path = data_dir / filename
        if not file_path.exists():
            continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if isinstance(data, dict):
                data = [data]

            for post in data:
                url = post.get('url', '')
                if url and url in seen_urls:
                    continue

                title = post.get('title', '').lower()
                content = post.get('content', '').lower()
                is_ai_related = any(kw in title or kw in content for kw in ai_keywords)

                if is_ai_related and url:
                    seen_urls.add(url)
                    all_posts.append(post)

        except Exception as e:
            pass

    return all_posts


def extract_keywords(posts):
    """æå–å’Œç»Ÿè®¡å…³é”®è¯"""
    # å®šä¹‰å…³é”®è¯ç±»åˆ«
    keyword_categories = {
        'AIæŠ€æœ¯': {
            'chatgpt': 'ChatGPT',
            'gpt': 'GPT',
            'ai': 'AI',
            'å¤§æ¨¡å‹': 'å¤§æ¨¡å‹',
            'äººå·¥æ™ºèƒ½': 'äººå·¥æ™ºèƒ½',
            'llm': 'LLM',
            'deepseek': 'DeepSeek',
            'claude': 'Claude',
            'gemini': 'Gemini'
        },
        'å²—ä½å½±å“': {
            'å¤±ä¸š': 'å¤±ä¸š',
            'è£å‘˜': 'è£å‘˜',
            'layoff': 'è£å‘˜',
            'å°±ä¸š': 'å°±ä¸š',
            'job': 'å·¥ä½œ',
            'career': 'èŒä¸š',
            'å²—ä½': 'å²—ä½',
            'employment': 'å°±ä¸š',
            'unemploy': 'å¤±ä¸š',
            'replace': 'æ›¿ä»£',
            'æ›¿ä»£': 'æ›¿ä»£',
            'å–ä»£': 'å–ä»£'
        },
        'æŠ€èƒ½éœ€æ±‚': {
            'æŠ€èƒ½': 'æŠ€èƒ½',
            'skill': 'æŠ€èƒ½',
            'å­¦ä¹ ': 'å­¦ä¹ ',
            'learn': 'å­¦ä¹ ',
            'è½¬å‹': 'è½¬å‹',
            'transition': 'è½¬å‹',
            'upskill': 'æŠ€èƒ½æå‡',
            'åŸ¹è®­': 'åŸ¹è®­',
            'training': 'åŸ¹è®­'
        },
        'ç¨‹åºå‘˜': {
            'ç¨‹åºå‘˜': 'ç¨‹åºå‘˜',
            'programmer': 'ç¨‹åºå‘˜',
            'developer': 'å¼€å‘è€…',
            'å¼€å‘': 'å¼€å‘',
            'coder': 'ç¼–ç¨‹è€…',
            'engineer': 'å·¥ç¨‹å¸ˆ',
            'software': 'è½¯ä»¶'
        },
        'æƒ…æ„Ÿè¯æ±‡': {
            'ç„¦è™‘': 'ç„¦è™‘',
            'anxiety': 'ç„¦è™‘',
            'worry': 'æ‹…å¿§',
            'æ‹…å¿ƒ': 'æ‹…å¿ƒ',
            'fear': 'ææƒ§',
            'hopeful': 'å¸Œæœ›',
            'optimistic': 'ä¹è§‚',
            'pessimistic': 'æ‚²è§‚'
        }
    }

    # ç»Ÿè®¡æ‰€æœ‰æ–‡æœ¬
    all_text = []
    for post in posts:
        text = post.get('title', '') + ' ' + post.get('content', '')
        # æ·»åŠ è¯„è®ºå†…å®¹ï¼ˆå‰100æ¡ï¼‰
        for comment in post.get('comments', [])[:100]:
            text += ' ' + comment.get('content', '')
        all_text.append(text.lower())

    combined_text = ' '.join(all_text)

    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å…³é”®è¯
    category_stats = {}
    for category, keywords in keyword_categories.items():
        stats = {}
        for keyword, display_name in keywords.items():
            count = combined_text.count(keyword.lower())
            if count > 0:
                stats[display_name] = count

        # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
        category_stats[category] = dict(sorted(stats.items(), key=lambda x: x[1], reverse=True))

    return category_stats


def analyze_sentiment(posts):
    """ç®€å•çš„æƒ…æ„Ÿåˆ†æ"""
    positive_words = ['good', 'great', 'excellent', 'amazing', 'helpful', 'useful',
                     'positive', 'opportunity', 'improve', 'better', 'å¥½', 'æ£’', 'æœ‰ç”¨',
                     'æœºä¼š', 'æå‡', 'è¿›æ­¥', 'hopeful', 'optimistic']

    negative_words = ['bad', 'terrible', 'awful', 'useless', 'worry', 'fear',
                     'replace', 'lose', 'job loss', 'å¤±ä¸š', 'æ‹…å¿ƒ', 'ç„¦è™‘',
                     'ç³Ÿç³•', 'ææƒ§', 'æ›¿ä»£', 'anxiety', 'pessimistic']

    neutral_words = ['think', 'maybe', 'possible', 'consider', 'å¯èƒ½', 'ä¹Ÿè®¸', 'è€ƒè™‘']

    sentiment_stats = {
        'positive': 0,
        'negative': 0,
        'neutral': 0,
        'total_posts': len(posts)
    }

    for post in posts:
        text = (post.get('title', '') + ' ' + post.get('content', '')).lower()

        pos_count = sum(text.count(word) for word in positive_words)
        neg_count = sum(text.count(word) for word in negative_words)
        neu_count = sum(text.count(word) for word in neutral_words)

        # æ ¹æ®è¯é¢‘åˆ¤æ–­å€¾å‘
        if pos_count > neg_count and pos_count > neu_count:
            sentiment_stats['positive'] += 1
        elif neg_count > pos_count:
            sentiment_stats['negative'] += 1
        else:
            sentiment_stats['neutral'] += 1

    return sentiment_stats


def extract_job_mentions(posts):
    """æå–è¢«æåŠçš„èŒä½ç±»å‹"""
    job_keywords = {
        'å‰ç«¯å¼€å‘': ['å‰ç«¯', 'frontend', 'front-end', 'react', 'vue', 'angular'],
        'åç«¯å¼€å‘': ['åç«¯', 'backend', 'back-end', 'server', 'api'],
        'å…¨æ ˆå¼€å‘': ['å…¨æ ˆ', 'fullstack', 'full-stack'],
        'ç®—æ³•å·¥ç¨‹å¸ˆ': ['ç®—æ³•', 'algorithm', 'ml engineer', 'machine learning'],
        'æ•°æ®åˆ†æ': ['æ•°æ®åˆ†æ', 'data analyst', 'data science'],
        'äº§å“ç»ç†': ['äº§å“ç»ç†', 'product manager', 'pm'],
        'UI/UXè®¾è®¡': ['ui', 'ux', 'è®¾è®¡å¸ˆ', 'designer'],
        'æµ‹è¯•å·¥ç¨‹å¸ˆ': ['æµ‹è¯•', 'test', 'qa', 'quality'],
        'è¿ç»´å·¥ç¨‹å¸ˆ': ['è¿ç»´', 'devops', 'sre', 'operations']
    }

    all_text = []
    for post in posts:
        text = post.get('title', '') + ' ' + post.get('content', '')
        for comment in post.get('comments', [])[:50]:
            text += ' ' + comment.get('content', '')
        all_text.append(text.lower())

    combined_text = ' '.join(all_text)

    job_stats = {}
    for job_type, keywords in job_keywords.items():
        count = sum(combined_text.count(kw.lower()) for kw in keywords)
        if count > 0:
            job_stats[job_type] = count

    # æŒ‰æåŠæ¬¡æ•°æ’åº
    return dict(sorted(job_stats.items(), key=lambda x: x[1], reverse=True))


def generate_report(keyword_stats, sentiment_stats, job_stats):
    """ç”Ÿæˆæ–‡æœ¬åˆ†ææŠ¥å‘Š"""
    output_dir = project_root / "outputs" / "results"
    output_dir.mkdir(parents=True, exist_ok=True)

    report_path = output_dir / "text_analysis.txt"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("å¤§æ¨¡å‹å¯¹ITè¡Œä¸šå½±å“ - æ–‡æœ¬åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")

        # å…³é”®è¯ç»Ÿè®¡
        f.write("ğŸ”‘ å…³é”®è¯ç»Ÿè®¡\n")
        f.write("-" * 80 + "\n\n")

        for category, keywords in keyword_stats.items():
            f.write(f"ã€{category}ã€‘\n")
            for word, count in list(keywords.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                f.write(f"  {word}: {count}æ¬¡\n")
            f.write("\n")

        # æƒ…æ„Ÿåˆ†æ
        f.write("ğŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†æ\n")
        f.write("-" * 80 + "\n")
        total = sentiment_stats['total_posts']
        f.write(f"ç§¯æå€¾å‘: {sentiment_stats['positive']}ç¯‡ ({sentiment_stats['positive']/total*100:.1f}%)\n")
        f.write(f"æ¶ˆæå€¾å‘: {sentiment_stats['negative']}ç¯‡ ({sentiment_stats['negative']/total*100:.1f}%)\n")
        f.write(f"ä¸­æ€§å€¾å‘: {sentiment_stats['neutral']}ç¯‡ ({sentiment_stats['neutral']/total*100:.1f}%)\n\n")

        # èŒä½æåŠ
        f.write("ğŸ’¼ èŒä½ç±»å‹æåŠé¢‘ç‡\n")
        f.write("-" * 80 + "\n")
        for job_type, count in job_stats.items():
            f.write(f"{job_type}: {count}æ¬¡æåŠ\n")
        f.write("\n")

        # åˆ†ææ´å¯Ÿ
        f.write("ğŸ’¡ åˆæ­¥æ´å¯Ÿ\n")
        f.write("-" * 80 + "\n")

        # æœ€å…³æ³¨çš„AIæŠ€æœ¯
        ai_tech = keyword_stats.get('AIæŠ€æœ¯', {})
        if ai_tech:
            top_ai = list(ai_tech.items())[0]
            f.write(f"1. æœ€å—å…³æ³¨çš„AIæŠ€æœ¯: {top_ai[0]} (æåŠ{top_ai[1]}æ¬¡)\n")

        # ä¸»è¦æ‹…å¿§
        job_impact = keyword_stats.get('å²—ä½å½±å“', {})
        if job_impact:
            top_concern = list(job_impact.items())[0]
            f.write(f"2. ä¸»è¦æ‹…å¿§: {top_concern[0]} (æåŠ{top_concern[1]}æ¬¡)\n")

        # æ•´ä½“æƒ…æ„Ÿ
        if sentiment_stats['negative'] > sentiment_stats['positive']:
            f.write(f"3. æ•´ä½“æƒ…æ„Ÿå€¾å‘: åæ¶ˆæ/æ‹…å¿§ (æ¶ˆæå æ¯”{sentiment_stats['negative']/total*100:.1f}%)\n")
        else:
            f.write(f"3. æ•´ä½“æƒ…æ„Ÿå€¾å‘: ç›¸å¯¹ç§¯æ/ç†æ€§ (ç§¯æå æ¯”{sentiment_stats['positive']/total*100:.1f}%)\n")

        f.write("\n")

    print(f"âœ… æ–‡æœ¬åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹æ–‡æœ¬åˆ†æ...")

    # åŠ è½½æ•°æ®
    print("\n1ï¸âƒ£ åŠ è½½æ•°æ®...")
    posts = load_all_posts()
    print(f"   æ‰¾åˆ° {len(posts)} ä¸ªå¸–å­")

    # å…³é”®è¯æå–
    print("\n2ï¸âƒ£ æå–å…³é”®è¯...")
    keyword_stats = extract_keywords(posts)

    # æƒ…æ„Ÿåˆ†æ
    print("\n3ï¸âƒ£ åˆ†ææƒ…æ„Ÿå€¾å‘...")
    sentiment_stats = analyze_sentiment(posts)

    # èŒä½æåŠ
    print("\n4ï¸âƒ£ ç»Ÿè®¡èŒä½æåŠ...")
    job_stats = extract_job_mentions(posts)

    # ç”ŸæˆæŠ¥å‘Š
    print("\n5ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š...")
    report_path = generate_report(keyword_stats, sentiment_stats, job_stats)

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“Š æ–‡æœ¬åˆ†ææ‘˜è¦")
    print("=" * 80)

    # æœ€çƒ­å…³é”®è¯
    all_keywords = []
    for category, keywords in keyword_stats.items():
        all_keywords.extend(keywords.items())
    all_keywords.sort(key=lambda x: x[1], reverse=True)

    print("\nğŸ”¥ çƒ­é—¨å…³é”®è¯ TOP 5:")
    for i, (word, count) in enumerate(all_keywords[:5], 1):
        print(f"  {i}. {word}: {count}æ¬¡")

    print(f"\nğŸ˜Š æƒ…æ„Ÿåˆ†å¸ƒ:")
    total = sentiment_stats['total_posts']
    print(f"  ç§¯æ: {sentiment_stats['positive']/total*100:.1f}% | "
          f"æ¶ˆæ: {sentiment_stats['negative']/total*100:.1f}% | "
          f"ä¸­æ€§: {sentiment_stats['neutral']/total*100:.1f}%")

    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³:")
    print(f"  {report_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
