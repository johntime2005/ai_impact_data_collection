#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢ç´¢æ€§æ•°æ®åˆ†æ - æ•°æ®æ¦‚è§ˆ
ç”Ÿæˆæ•°æ®åŸºç¡€ç»Ÿè®¡å’Œè´¨é‡æŠ¥å‘Š
"""

import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))


def load_all_posts():
    """åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„å¸–å­æ•°æ®å¹¶å»é‡"""
    data_dir = project_root / "data" / "raw"

    valid_files = [
        "posts_20251121_093153.json",
        "posts_20251121_091738.json",
        "merged_posts_20251121_133607.json",
        "reddit_post_2.json",
        "reddit_post_6.json",
        "reddit_post_7.json",
        "reddit_post_10.json"
    ]

    all_posts = []
    seen_urls = set()

    # AI/ITç›¸å…³å…³é”®è¯
    ai_keywords = [
        'chatgpt', 'gpt', 'ai', 'å¤§æ¨¡å‹', 'äººå·¥æ™ºèƒ½', 'llm',
        'ç¨‹åºå‘˜', 'it', 'å¼€å‘', 'å¤±ä¸š', 'å²—ä½', 'æŠ€èƒ½',
        'èŒä¸š', 'programmer', 'developer', 'job', 'deepseek',
        'software engineer', 'coding', 'employment'
    ]

    for filename in valid_files:
        file_path = data_dir / filename
        if not file_path.exists():
            continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # ç»Ÿä¸€ä¸ºåˆ—è¡¨æ ¼å¼
            if isinstance(data, dict):
                data = [data]

            for post in data:
                url = post.get('url', '')

                # å»é‡
                if url and url in seen_urls:
                    continue

                # æ£€æŸ¥ç›¸å…³æ€§
                title = post.get('title', '').lower()
                content = post.get('content', '').lower()
                is_ai_related = any(kw in title or kw in content for kw in ai_keywords)

                if is_ai_related and url:
                    seen_urls.add(url)
                    all_posts.append(post)

        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å– {filename}: {e}")

    return all_posts


def analyze_basic_stats(posts):
    """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
    stats = {
        'total_posts': len(posts),
        'total_comments': sum(p.get('comment_count', 0) for p in posts),
        'avg_comments': 0,
        'posts_with_100plus_comments': 0,
        'platforms': defaultdict(int),
        'time_distribution': defaultdict(int)
    }

    if stats['total_posts'] > 0:
        stats['avg_comments'] = stats['total_comments'] / stats['total_posts']

    for post in posts:
        # è¯„è®ºæ•°ç»Ÿè®¡
        if post.get('comment_count', 0) >= 100:
            stats['posts_with_100plus_comments'] += 1

        # å¹³å°åˆ†å¸ƒ
        platform = post.get('platform', 'unknown')
        stats['platforms'][platform] += 1

        # æ—¶é—´åˆ†å¸ƒï¼ˆæå–å¹´ä»½ï¼‰
        created_at = post.get('created_at', '')
        year = extract_year(created_at)
        if year:
            stats['time_distribution'][year] += 1

    return stats


def extract_year(date_str):
    """ä»å„ç§æ—¥æœŸæ ¼å¼ä¸­æå–å¹´ä»½"""
    if not date_str:
        return None

    # åŒ¹é… 2023, 2024, 2025 ç­‰
    match = re.search(r'(202[2-5])', str(date_str))
    if match:
        return match.group(1)

    return None


def analyze_content(posts):
    """å†…å®¹åˆ†æ"""
    all_titles = ' '.join([p.get('title', '') for p in posts])
    all_content = ' '.join([p.get('content', '')[:500] for p in posts])  # åªå–å‰500å­—ç¬¦

    # å…³é”®è¯ç»Ÿè®¡
    keywords = {
        'AI/ChatGPT': ['chatgpt', 'gpt', 'ai', 'å¤§æ¨¡å‹', 'äººå·¥æ™ºèƒ½'],
        'å°±ä¸šç›¸å…³': ['å¤±ä¸š', 'å²—ä½', 'èŒä¸š', 'job', 'employment', 'career'],
        'æŠ€èƒ½ç›¸å…³': ['æŠ€èƒ½', 'skill', 'å­¦ä¹ ', 'learn', 'è½¬å‹'],
        'ç¨‹åºå‘˜': ['ç¨‹åºå‘˜', 'programmer', 'developer', 'å¼€å‘', 'coder']
    }

    keyword_counts = {}
    combined_text = (all_titles + ' ' + all_content).lower()

    for category, words in keywords.items():
        count = sum(combined_text.count(word) for word in words)
        keyword_counts[category] = count

    return keyword_counts


def generate_report(posts, stats, keywords):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    output_dir = project_root / "outputs" / "results"
    output_dir.mkdir(parents=True, exist_ok=True)

    report_path = output_dir / "data_summary.txt"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("å¤§æ¨¡å‹å¯¹ITè¡Œä¸šå½±å“ - æ•°æ®æ¦‚è§ˆæŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")

        # åŸºç¡€ç»Ÿè®¡
        f.write("ğŸ“Š åŸºç¡€ç»Ÿè®¡\n")
        f.write("-" * 80 + "\n")
        f.write(f"æ€»å¸–å­æ•°: {stats['total_posts']}\n")
        f.write(f"æ€»è¯„è®ºæ•°: {stats['total_comments']}\n")
        f.write(f"å¹³å‡è¯„è®ºæ•°: {stats['avg_comments']:.1f}\n")
        f.write(f"è¯„è®ºâ‰¥100çš„å¸–å­æ•°: {stats['posts_with_100plus_comments']}\n")
        f.write(f"è·ç¦»ä½œä¸šè¦æ±‚(â‰¥18): {'âœ… è¾¾æ ‡' if stats['posts_with_100plus_comments'] >= 18 else f'âš ï¸  è¿˜éœ€{18 - stats['posts_with_100plus_comments']}ä¸ª'}\n\n")

        # å¹³å°åˆ†å¸ƒ
        f.write("ğŸ“ å¹³å°åˆ†å¸ƒ\n")
        f.write("-" * 80 + "\n")
        for platform, count in stats['platforms'].items():
            f.write(f"{platform}: {count}ä¸ªå¸–å­\n")
        f.write("\n")

        # æ—¶é—´åˆ†å¸ƒ
        f.write("ğŸ“… æ—¶é—´åˆ†å¸ƒ\n")
        f.write("-" * 80 + "\n")
        for year in sorted(stats['time_distribution'].keys()):
            count = stats['time_distribution'][year]
            f.write(f"{year}å¹´: {count}ä¸ªå¸–å­\n")
        f.write("\n")

        # å…³é”®è¯ç»Ÿè®¡
        f.write("ğŸ”‘ å…³é”®è¯ç»Ÿè®¡\n")
        f.write("-" * 80 + "\n")
        for category, count in keywords.items():
            f.write(f"{category}: {count}æ¬¡æåŠ\n")
        f.write("\n")

        # å¸–å­åˆ—è¡¨
        f.write("ğŸ“ å¸–å­åˆ—è¡¨\n")
        f.write("-" * 80 + "\n")
        for i, post in enumerate(posts, 1):
            f.write(f"\n{i}. [{post.get('platform', 'N/A')}] {post.get('title', 'N/A')}\n")
            f.write(f"   è¯„è®ºæ•°: {post.get('comment_count', 0)} | æ—¶é—´: {post.get('created_at', 'N/A')}\n")
            f.write(f"   URL: {post.get('url', 'N/A')}\n")

    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹æ•°æ®åˆ†æ...")

    # åŠ è½½æ•°æ®
    print("\n1ï¸âƒ£ åŠ è½½æ•°æ®...")
    posts = load_all_posts()
    print(f"   æ‰¾åˆ° {len(posts)} ä¸ªAI/ITç›¸å…³å¸–å­")

    # åŸºç¡€ç»Ÿè®¡
    print("\n2ï¸âƒ£ åŸºç¡€ç»Ÿè®¡åˆ†æ...")
    stats = analyze_basic_stats(posts)

    # å†…å®¹åˆ†æ
    print("\n3ï¸âƒ£ å†…å®¹å…³é”®è¯åˆ†æ...")
    keywords = analyze_content(posts)

    # ç”ŸæˆæŠ¥å‘Š
    print("\n4ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š...")
    report_path = generate_report(posts, stats, keywords)

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“Š æ•°æ®æ¦‚è§ˆæ‘˜è¦")
    print("=" * 80)
    print(f"æ€»å¸–å­æ•°: {stats['total_posts']}")
    print(f"ç¬¦åˆè¦æ±‚(è¯„è®ºâ‰¥100): {stats['posts_with_100plus_comments']}")
    print(f"æ•°æ®å®Œæ•´åº¦: {stats['posts_with_100plus_comments']/18*100:.1f}%")
    print("\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³:")
    print(f"  {report_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
