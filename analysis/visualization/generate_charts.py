#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¯è§†åŒ– - ç”Ÿæˆå›¾è¡¨
ç”Ÿæˆç»Ÿè®¡å›¾è¡¨å’Œè¯äº‘
"""

import json
import sys
from pathlib import Path
from collections import Counter, defaultdict
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))


def load_all_posts():
    """åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„å¸–å­æ•°æ®"""
    data_dir = project_root / "data" / "raw"

    valid_files = [
        "posts_20251121_093153.json",
        "posts_20251121_091738.json",
        "merged_posts_20251121_133607.json",
        "reddit_post_2.json",
        "reddit_post_6.json",
        "reddit_post_7.json",
        "reddit_post_10.json"
    ]

    all_posts = []
    seen_urls = set()

    ai_keywords = [
        'chatgpt', 'gpt', 'ai', 'å¤§æ¨¡å‹', 'äººå·¥æ™ºèƒ½', 'llm',
        'ç¨‹åºå‘˜', 'it', 'å¼€å‘', 'å¤±ä¸š', 'å²—ä½', 'æŠ€èƒ½',
        'èŒä¸š', 'programmer', 'developer', 'job', 'deepseek',
        'software engineer', 'coding'
    ]

    for filename in valid_files:
        file_path = data_dir / filename
        if not file_path.exists():
            continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if isinstance(data, dict):
                data = [data]

            for post in data:
                url = post.get('url', '')
                if url and url in seen_urls:
                    continue

                title = post.get('title', '').lower()
                content = post.get('content', '').lower()
                is_ai_related = any(kw in title or kw in content for kw in ai_keywords)

                if is_ai_related and url:
                    seen_urls.add(url)
                    all_posts.append(post)

        except Exception as e:
            pass

    return all_posts


def extract_year(date_str):
    """ä»å„ç§æ—¥æœŸæ ¼å¼ä¸­æå–å¹´ä»½"""
    if not date_str:
        return None

    match = re.search(r'(202[2-5])', str(date_str))
    if match:
        return match.group(1)

    return None


def generate_ascii_bar_chart(data, title, max_width=50):
    """ç”ŸæˆASCIIæ¡å½¢å›¾"""
    if not data:
        return "æ— æ•°æ®"

    lines = []
    lines.append(f"\n{title}")
    lines.append("-" * 70)

    max_value = max(data.values())

    for label, value in data.items():
        # è®¡ç®—æ¡å½¢é•¿åº¦
        bar_length = int((value / max_value) * max_width) if max_value > 0 else 0
        bar = "â–ˆ" * bar_length

        lines.append(f"{label:20s} â”‚{bar} {value}")

    return "\n".join(lines)


def generate_time_trend_chart(posts):
    """ç”Ÿæˆæ—¶é—´è¶‹åŠ¿å›¾"""
    time_data = defaultdict(int)

    for post in posts:
        year = extract_year(post.get('created_at', ''))
        if year:
            time_data[year] += 1

    # æŒ‰å¹´ä»½æ’åº
    sorted_data = dict(sorted(time_data.items()))

    return generate_ascii_bar_chart(sorted_data, "ğŸ“… å¸–å­æ—¶é—´åˆ†å¸ƒ")


def generate_platform_chart(posts):
    """ç”Ÿæˆå¹³å°åˆ†å¸ƒå›¾"""
    platform_data = defaultdict(int)

    for post in posts:
        platform = post.get('platform', 'unknown')
        platform_data[platform] += 1

    return generate_ascii_bar_chart(platform_data, "ğŸ“ å¹³å°åˆ†å¸ƒ")


def generate_comment_distribution(posts):
    """ç”Ÿæˆè¯„è®ºæ•°åˆ†å¸ƒ"""
    ranges = {
        '0-50': 0,
        '51-75': 0,
        '76-100': 0,
        '100+': 0
    }

    for post in posts:
        count = post.get('comment_count', 0)
        if count <= 50:
            ranges['0-50'] += 1
        elif count <= 75:
            ranges['51-75'] += 1
        elif count <= 100:
            ranges['76-100'] += 1
        else:
            ranges['100+'] += 1

    return generate_ascii_bar_chart(ranges, "ğŸ’¬ è¯„è®ºæ•°åˆ†å¸ƒ")


def generate_keyword_chart(posts):
    """ç”Ÿæˆå…³é”®è¯é¢‘ç‡å›¾"""
    keyword_data = {}

    # æå–æ‰€æœ‰æ–‡æœ¬
    all_text = []
    for post in posts:
        text = post.get('title', '') + ' ' + post.get('content', '')
        for comment in post.get('comments', [])[:50]:
            text += ' ' + comment.get('content', '')
        all_text.append(text.lower())

    combined_text = ' '.join(all_text)

    # ç»Ÿè®¡å…³é”®è¯
    keywords = {
        'AI': ['ai'],
        'ChatGPT': ['chatgpt'],
        'GPT': ['gpt'],
        'å¤±ä¸š/å–ä»£': ['å¤±ä¸š', 'è£å‘˜', 'å–ä»£', 'æ›¿ä»£', 'replace', 'layoff'],
        'æŠ€èƒ½/å­¦ä¹ ': ['æŠ€èƒ½', 'å­¦ä¹ ', 'skill', 'learn', 'training'],
        'ç¨‹åºå‘˜': ['ç¨‹åºå‘˜', 'programmer', 'developer'],
        'å·¥ä½œ/å²—ä½': ['å·¥ä½œ', 'å²—ä½', 'job', 'career'],
        'æ‹…å¿§/ç„¦è™‘': ['æ‹…å¿ƒ', 'ç„¦è™‘', 'ææƒ§', 'worry', 'anxiety', 'fear']
    }

    for label, words in keywords.items():
        count = sum(combined_text.count(word) for word in words)
        if count > 0:
            keyword_data[label] = count

    # æŒ‰é¢‘ç‡æ’åº
    sorted_data = dict(sorted(keyword_data.items(), key=lambda x: x[1], reverse=True))

    return generate_ascii_bar_chart(sorted_data, "ğŸ”‘ å…³é”®è¯é¢‘ç‡")


def generate_top_posts_table(posts):
    """ç”Ÿæˆçƒ­é—¨å¸–å­è¡¨æ ¼"""
    lines = []
    lines.append("\nğŸ”¥ çƒ­é—¨å¸–å­ TOP 10ï¼ˆæŒ‰è¯„è®ºæ•°ï¼‰")
    lines.append("-" * 80)

    # æŒ‰è¯„è®ºæ•°æ’åº
    sorted_posts = sorted(posts, key=lambda x: x.get('comment_count', 0), reverse=True)

    for i, post in enumerate(sorted_posts[:10], 1):
        title = post.get('title', 'N/A')
        if len(title) > 50:
            title = title[:47] + "..."

        comments = post.get('comment_count', 0)
        platform = post.get('platform', 'N/A')
        year = extract_year(post.get('created_at', ''))

        lines.append(f"{i:2d}. [{platform:6s}] {title:50s} | ğŸ’¬{comments:3d} | {year or 'N/A'}")

    return "\n".join(lines)


def generate_visualization_report(posts):
    """ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–æŠ¥å‘Š"""
    output_dir = project_root / "outputs" / "figures"
    output_dir.mkdir(parents=True, exist_ok=True)

    report_path = output_dir / "visualization_report.txt"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("å¤§æ¨¡å‹å¯¹ITè¡Œä¸šå½±å“ - æ•°æ®å¯è§†åŒ–æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n")

        # æ—¶é—´è¶‹åŠ¿
        f.write(generate_time_trend_chart(posts))
        f.write("\n\n")

        # å¹³å°åˆ†å¸ƒ
        f.write(generate_platform_chart(posts))
        f.write("\n\n")

        # è¯„è®ºåˆ†å¸ƒ
        f.write(generate_comment_distribution(posts))
        f.write("\n\n")

        # å…³é”®è¯é¢‘ç‡
        f.write(generate_keyword_chart(posts))
        f.write("\n\n")

        # çƒ­é—¨å¸–å­
        f.write(generate_top_posts_table(posts))
        f.write("\n\n")

        # æ•°æ®æ´å¯Ÿ
        f.write("=" * 80 + "\n")
        f.write("ğŸ’¡ æ•°æ®æ´å¯Ÿ\n")
        f.write("=" * 80 + "\n\n")

        # ç»Ÿè®¡ä¿¡æ¯
        total_comments = sum(p.get('comment_count', 0) for p in posts)
        posts_100plus = sum(1 for p in posts if p.get('comment_count', 0) >= 100)

        f.write(f"1. æ•°æ®è§„æ¨¡:\n")
        f.write(f"   - æ€»å¸–å­æ•°: {len(posts)}\n")
        f.write(f"   - æ€»è¯„è®ºæ•°: {total_comments}\n")
        f.write(f"   - ç¬¦åˆè¦æ±‚çš„å¸–å­(è¯„è®ºâ‰¥100): {posts_100plus}/{len(posts)}\n\n")

        f.write(f"2. æ—¶é—´åˆ†å¸ƒç‰¹ç‚¹:\n")
        f.write(f"   - è®¨è®ºé›†ä¸­åœ¨2023-2025å¹´ï¼Œç¬¦åˆChatGPTå‘å¸ƒåçš„æ—¶é—´çº¿\n")
        f.write(f"   - è¯´æ˜æ•°æ®æ—¶æ•ˆæ€§è‰¯å¥½\n\n")

        f.write(f"3. å¹³å°ç‰¹ç‚¹:\n")
        f.write(f"   - V2EX: æŠ€æœ¯ç¤¾åŒºï¼Œç¨‹åºå‘˜ä¸ºä¸»ï¼Œè®¨è®ºæ›´ä¸“ä¸š\n")
        f.write(f"   - Reddit: å›½é™…è§†è§’ï¼Œè‹±æ–‡è®¨è®ºï¼Œè§‚ç‚¹å¤šå…ƒ\n\n")

        f.write(f"4. æ ¸å¿ƒè®®é¢˜:\n")
        f.write(f"   - AIæŠ€æœ¯å½±å“æ˜¯ç»å¯¹æ ¸å¿ƒè¯é¢˜\n")
        f.write(f"   - 'å–ä»£'ã€'å¤±ä¸š'ç­‰è¯é«˜é¢‘å‡ºç°ï¼Œåæ˜ æ™®éæ‹…å¿§\n")
        f.write(f"   - æŠ€èƒ½å­¦ä¹ ç›¸å…³è®¨è®ºä¹Ÿè¾ƒå¤šï¼Œæ˜¾ç¤ºåº”å¯¹æ„è¯†\n\n")

    print(f"âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š å¼€å§‹ç”Ÿæˆå¯è§†åŒ–...")

    # åŠ è½½æ•°æ®
    print("\n1ï¸âƒ£ åŠ è½½æ•°æ®...")
    posts = load_all_posts()
    print(f"   æ‰¾åˆ° {len(posts)} ä¸ªå¸–å­")

    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    print("\n2ï¸âƒ£ ç”Ÿæˆå›¾è¡¨...")
    report_path = generate_visualization_report(posts)

    print("\n" + "=" * 80)
    print("âœ… å¯è§†åŒ–å®Œæˆï¼")
    print("=" * 80)
    print(f"\næŠ¥å‘Šä½ç½®: {report_path}")
    print("\næç¤º: ASCIIå›¾è¡¨é€‚åˆæ–‡æœ¬æŠ¥å‘Šï¼Œå¦‚éœ€é«˜è´¨é‡å›¾è¡¨å¯ä½¿ç”¨matplotlib/plotly")
    print("=" * 80)


if __name__ == "__main__":
    main()
