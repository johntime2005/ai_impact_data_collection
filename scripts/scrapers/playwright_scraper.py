#!/usr/bin/env python3
"""
ä½¿ç”¨Playwrightæµè§ˆå™¨è‡ªåŠ¨åŒ–é‡‡é›†çŸ¥ä¹æ•°æ®

ç”¨æ³•ï¼š
    pixi run python playwright_scraper.py
"""

import json
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from playwright.async_api import async_playwright, Page, Browser
import re


class PlaywrightZhihuScraper:
    """ä½¿ç”¨Playwrighté‡‡é›†çŸ¥ä¹æ•°æ®"""

    def __init__(self, config_path: str = "config/target_urls.json"):
        self.config_path = Path(config_path)
        self.output_dir = Path("data/raw")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    async def scrape_zhihu_question(self, page: Page, url: str) -> Optional[Dict]:
        """é‡‡é›†å•ä¸ªçŸ¥ä¹é—®é¢˜"""
        try:
            print(f"\nğŸ” æ­£åœ¨é‡‡é›†: {url}")

            # è®¿é—®é¡µé¢
            await page.goto(url, wait_until="networkidle", timeout=60000)

            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_timeout(3000)

            # æå–é—®é¢˜ID
            question_id = url.split("/")[-1]

            # æå–é—®é¢˜æ ‡é¢˜
            try:
                title_element = await page.query_selector("h1.QuestionHeader-title")
                title = await title_element.inner_text() if title_element else "æœªçŸ¥æ ‡é¢˜"
            except Exception as e:
                print(f"âš ï¸ æå–æ ‡é¢˜å¤±è´¥: {e}")
                title = "æœªçŸ¥æ ‡é¢˜"

            # æå–é—®é¢˜æè¿°
            try:
                detail_element = await page.query_selector(".QuestionRichText")
                detail = await detail_element.inner_text() if detail_element else ""
            except:
                detail = ""

            # æå–å›ç­”æ•°
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                answer_count_text = None
                selectors = [
                    ".List-headerText span",
                    "h4.List-header-title",
                    ".QuestionAnswers-answerCount"
                ]

                for selector in selectors:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        # æå–æ•°å­—
                        match = re.search(r'(\d+)', text)
                        if match:
                            answer_count_text = match.group(1)
                            break

                answer_count = int(answer_count_text) if answer_count_text else 0
            except Exception as e:
                print(f"âš ï¸ æå–å›ç­”æ•°å¤±è´¥: {e}")
                answer_count = 0

            print(f"   æ ‡é¢˜: {title}")
            print(f"   å›ç­”æ•°: {answer_count}")

            # æ£€æŸ¥å›ç­”æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚
            if answer_count < 100:
                print(f"âŒ å›ç­”æ•°ä¸è¶³100: {answer_count}")
                return None

            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå›ç­”
            print(f"ğŸ“œ æ­£åœ¨åŠ è½½å›ç­”å†…å®¹...")
            await self._scroll_to_load_answers(page, max_scrolls=10)

            # æå–å›ç­”åˆ—è¡¨
            answers = []
            try:
                answer_elements = await page.query_selector_all(".List-item")

                for i, answer_elem in enumerate(answer_elements[:100]):  # æœ€å¤šé‡‡é›†100æ¡å›ç­”
                    try:
                        # æå–å›ç­”è€…
                        author_elem = await answer_elem.query_selector(".AuthorInfo-name")
                        author = await author_elem.inner_text() if author_elem else "åŒ¿åç”¨æˆ·"

                        # æå–å›ç­”å†…å®¹
                        content_elem = await answer_elem.query_selector(".RichContent-inner")
                        content = await content_elem.inner_text() if content_elem else ""

                        # æå–ç‚¹èµæ•°
                        vote_elem = await answer_elem.query_selector(".VoteButton--up")
                        vote_text = await vote_elem.inner_text() if vote_elem else "0"
                        vote_count = self._extract_number(vote_text)

                        # æå–æ—¶é—´
                        time_elem = await answer_elem.query_selector(".ContentItem-time")
                        created_time = await time_elem.get_attribute("datetime") if time_elem else ""

                        if content:  # åªæ·»åŠ æœ‰å†…å®¹çš„å›ç­”
                            answers.append({
                                "author": author.strip(),
                                "content": content.strip()[:1000],  # é™åˆ¶é•¿åº¦
                                "vote_count": vote_count,
                                "created_at": created_time
                            })
                    except Exception as e:
                        continue

            except Exception as e:
                print(f"âš ï¸ æå–å›ç­”åˆ—è¡¨å¤±è´¥: {e}")

            print(f"âœ… æˆåŠŸæå– {len(answers)} æ¡å›ç­”")

            # æ„é€ æ•°æ®å¯¹è±¡
            post_data = {
                "platform": "zhihu",
                "post_type": "question",
                "post_id": question_id,
                "url": url,
                "title": title.strip(),
                "content": detail.strip(),
                "author": "",  # çŸ¥ä¹é—®é¢˜æ²¡æœ‰å•ä¸€ä½œè€…
                "created_at": datetime.now().isoformat(),
                "scraped_at": datetime.now().isoformat(),
                "view_count": 0,
                "like_count": 0,
                "comment_count": answer_count,
                "share_count": 0,
                "comments": answers,
                "is_relevant": True,
                "relevance_note": "Playwrighté‡‡é›†"
            }

            return post_data

        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥ {url}: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def _scroll_to_load_answers(self, page: Page, max_scrolls: int = 10):
        """æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå›ç­”"""
        for i in range(max_scrolls):
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1500)

    def _extract_number(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        # å¤„ç† "1.2 ä¸‡" è¿™ç§æ ¼å¼
        if "ä¸‡" in text:
            match = re.search(r'([\d.]+)\s*ä¸‡', text)
            if match:
                return int(float(match.group(1)) * 10000)

        # å¤„ç†æ™®é€šæ•°å­—
        match = re.search(r'\d+', text.replace(',', ''))
        return int(match.group()) if match else 0

    async def scrape_all(self):
        """é‡‡é›†æ‰€æœ‰URL"""
        # è¯»å–é…ç½®
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        zhihu_posts = config.get('zhihu_posts', [])

        if not zhihu_posts:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°çŸ¥ä¹URLé…ç½®")
            return

        print(f"ğŸ“š æ‰¾åˆ° {len(zhihu_posts)} ä¸ªçŸ¥ä¹URL")

        # å¯åŠ¨æµè§ˆå™¨
        async with async_playwright() as p:
            print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
            browser = await p.chromium.launch(
                headless=False,  # éæ— å¤´æ¨¡å¼ï¼Œå¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œ
                args=['--disable-blink-features=AutomationControlled']
            )

            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )

            page = await context.new_page()

            # é‡‡é›†æ‰€æœ‰URL
            all_posts = []
            success_count = 0
            fail_count = 0

            for i, post_info in enumerate(zhihu_posts, 1):
                url = post_info.get('url')
                if not url:
                    continue

                print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                print(f"è¿›åº¦: {i}/{len(zhihu_posts)}")

                post_data = await self.scrape_zhihu_question(page, url)

                if post_data:
                    all_posts.append(post_data)
                    success_count += 1
                    print(f"âœ… æˆåŠŸ ({success_count}/{i})")
                else:
                    fail_count += 1
                    print(f"âŒ å¤±è´¥ ({fail_count}/{i})")

                # ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                await page.wait_for_timeout(2000)

            await browser.close()

        # ä¿å­˜æ•°æ®
        if all_posts:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.output_dir / f"posts_playwright_{timestamp}.json"

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_posts, f, ensure_ascii=False, indent=2)

            print(f"\n{'='*60}")
            print(f"âœ… é‡‡é›†å®Œæˆï¼")
            print(f"{'='*60}")
            print(f"æˆåŠŸ: {success_count} ä¸ª")
            print(f"å¤±è´¥: {fail_count} ä¸ª")
            print(f"æ€»è®¡: {len(all_posts)} æ¡æ•°æ®")
            print(f"ä¿å­˜åˆ°: {output_file}")
            print(f"{'='*60}")
        else:
            print("\nâŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ•°æ®")


async def main():
    scraper = PlaywrightZhihuScraper()
    await scraper.scrape_all()


if __name__ == "__main__":
    asyncio.run(main())
