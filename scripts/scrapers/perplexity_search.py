#!/usr/bin/env python3
"""
ä½¿ç”¨Perplexity APIæœç´¢ç¬¦åˆè¦æ±‚çš„è®¨è®ºURL

ç”¨æ³•ï¼š
    export PERPLEXITY_API_KEY="your-api-key"
    python perplexity_search.py

æˆ–è€…ï¼š
    python perplexity_search.py --api-key "your-api-key"
"""

import os
import sys
import re
import json
import argparse
from pathlib import Path
from typing import List, Dict, Optional
import requests
from datetime import datetime


class PerplexityURLSearcher:
    """ä½¿ç”¨Perplexity APIæœç´¢ç¬¦åˆè¦æ±‚çš„è®¨è®ºURL"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.api_url = "https://api.perplexity.ai/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def search_urls(self, target_count: int = 20) -> Dict[str, List[Dict]]:
        """
        æœç´¢ç¬¦åˆè¦æ±‚çš„URL

        Args:
            target_count: ç›®æ ‡URLæ•°é‡

        Returns:
            åŒ…å«zhihuå’Œv2ex URLåˆ—è¡¨çš„å­—å…¸
        """
        print(f"\nğŸ” å¼€å§‹ä½¿ç”¨Perplexity APIæœç´¢URL... ç›®æ ‡æ•°é‡: {target_count}")

        # æ„é€ æœç´¢prompt
        prompt = self._build_search_prompt(target_count)

        # è°ƒç”¨API
        response = self._call_api(prompt)

        # è§£æç»“æœ
        urls = self._parse_response(response)

        return urls

    def _build_search_prompt(self, count: int) -> str:
        """æ„é€ æœç´¢prompt"""
        return f"""è¯·å¸®æˆ‘åœ¨çŸ¥ä¹å’ŒV2EXä¸Šæ‰¾{count}ä¸ªå…³äº"ChatGPT/å¤§æ¨¡å‹/AIå¯¹ITè¡Œä¸šå½±å“"çš„çƒ­é—¨è®¨è®ºå¸–å­ã€‚

è¦æ±‚ï¼š
1. è®¨è®ºä¸»é¢˜ï¼šå¿…é¡»æ˜¯å…³äºAI/ChatGPT/å¤§æ¨¡å‹å¯¹ITä»ä¸šè€…ã€ç¨‹åºå‘˜çš„å½±å“ï¼ˆåŒ…æ‹¬å°±ä¸šã€æŠ€èƒ½éœ€æ±‚ã€èŒä¸šå‘å±•ç­‰ï¼‰
2. è¯„è®ºæ•°é‡ï¼šæ¯ä¸ªå¸–å­å¿…é¡»æœ‰â‰¥100æ¡å›ç­”/è¯„è®º
3. å¹³å°ï¼šçŸ¥ä¹é—®é¢˜(zhihu.com/question/)æˆ–V2EXè®¨è®ºå¸–(v2ex.com/t/)
4. æ—¶æ•ˆæ€§ï¼šä¼˜å…ˆé€‰æ‹©2023-2024å¹´çš„è®¨è®º
5. çƒ­åº¦ï¼šé€‰æ‹©è®¨è®ºçƒ­åº¦é«˜ã€äº’åŠ¨å¤šçš„å¸–å­

è¯·ç›´æ¥ç»™å‡ºURLåˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
çŸ¥ä¹ï¼š
- https://www.zhihu.com/question/xxxxx (æ ‡é¢˜ï¼šxxxï¼Œå›ç­”æ•°ï¼šxxx)
- https://www.zhihu.com/question/xxxxx (æ ‡é¢˜ï¼šxxxï¼Œå›ç­”æ•°ï¼šxxx)

V2EXï¼š
- https://v2ex.com/t/xxxxx (æ ‡é¢˜ï¼šxxxï¼Œå›å¤æ•°ï¼šxxx)
- https://v2ex.com/t/xxxxx (æ ‡é¢˜ï¼šxxxï¼Œå›å¤æ•°ï¼šxxx)

è¯·ç¡®ä¿æ¯ä¸ªURLéƒ½æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œå¹¶ä¸”è¯„è®ºæ•°â‰¥100ã€‚"""

    def _call_api(self, prompt: str, model: str = "sonar-pro") -> str:
        """
        è°ƒç”¨Perplexity API

        Args:
            prompt: æœç´¢prompt
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆsonaræ¨¡å‹æ”¯æŒåœ¨çº¿æœç´¢ï¼‰

        Returns:
            APIå“åº”å†…å®¹
        """
        print(f"\nğŸ“¡ æ­£åœ¨è°ƒç”¨Perplexity API (æ¨¡å‹: {model})...")

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡äº’è”ç½‘å†…å®¹æœç´¢åŠ©æ‰‹ï¼Œæ“…é•¿åœ¨çŸ¥ä¹ã€V2EXç­‰å¹³å°ä¸Šæ‰¾åˆ°é«˜è´¨é‡çš„è®¨è®ºå¸–å­ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.2,  # é™ä½éšæœºæ€§ï¼Œæé«˜å‡†ç¡®æ€§
            "max_tokens": 2000
        }

        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()

            result = response.json()
            content = result['choices'][0]['message']['content']

            print(f"âœ… APIè°ƒç”¨æˆåŠŸï¼å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
            return content

        except requests.exceptions.RequestException as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            if hasattr(e.response, 'text'):
                print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
            raise

    def _parse_response(self, response: str) -> Dict[str, List[Dict]]:
        """
        è§£æAPIå“åº”ï¼Œæå–URL

        Args:
            response: APIå“åº”å†…å®¹

        Returns:
            åŒ…å«zhihuå’Œv2ex URLçš„å­—å…¸
        """
        print("\nğŸ“ è§£æAPIå“åº”...")

        urls = {
            "zhihu": [],
            "v2ex": []
        }

        # æ­£åˆ™è¡¨è¾¾å¼æå–çŸ¥ä¹URL
        zhihu_pattern = r'https://www\.zhihu\.com/question/(\d+)'
        zhihu_matches = re.finditer(zhihu_pattern, response)

        for match in zhihu_matches:
            question_id = match.group(1)
            url = f"https://www.zhihu.com/question/{question_id}"

            # å°è¯•æå–æ ‡é¢˜å’Œè¯„è®ºæ•°
            title, comment_count = self._extract_metadata(response, url)

            urls["zhihu"].append({
                "url": url,
                "question_id": question_id,
                "title": title,
                "estimated_comments": comment_count,
                "source": "perplexity_api",
                "search_date": datetime.now().isoformat()
            })

        # æ­£åˆ™è¡¨è¾¾å¼æå–V2EX URL
        v2ex_pattern = r'https://(?:www\.)?v2ex\.com/t/(\d+)'
        v2ex_matches = re.finditer(v2ex_pattern, response)

        for match in v2ex_matches:
            topic_id = match.group(1)
            url = f"https://v2ex.com/t/{topic_id}"

            # å°è¯•æå–æ ‡é¢˜å’Œè¯„è®ºæ•°
            title, comment_count = self._extract_metadata(response, url)

            urls["v2ex"].append({
                "url": url,
                "topic_id": topic_id,
                "title": title,
                "estimated_comments": comment_count,
                "source": "perplexity_api",
                "search_date": datetime.now().isoformat()
            })

        # å»é‡
        urls["zhihu"] = self._deduplicate_urls(urls["zhihu"], "question_id")
        urls["v2ex"] = self._deduplicate_urls(urls["v2ex"], "topic_id")

        print(f"âœ… æ‰¾åˆ° {len(urls['zhihu'])} ä¸ªçŸ¥ä¹URL")
        print(f"âœ… æ‰¾åˆ° {len(urls['v2ex'])} ä¸ªV2EX URL")
        print(f"ğŸ“Š æ€»è®¡: {len(urls['zhihu']) + len(urls['v2ex'])} ä¸ªURL")

        return urls

    def _extract_metadata(self, text: str, url: str) -> tuple[Optional[str], Optional[int]]:
        """ä»å“åº”æ–‡æœ¬ä¸­æå–URLçš„å…ƒæ•°æ®"""
        # å°è¯•æ‰¾åˆ°URLæ‰€åœ¨è¡Œ
        for line in text.split('\n'):
            if url in line:
                # æå–æ ‡é¢˜
                title_match = re.search(r'æ ‡é¢˜[ï¼š:](.*?)(?:ï¼Œ|,|å›ç­”æ•°|å›å¤æ•°|$)', line)
                title = title_match.group(1).strip() if title_match else None

                # æå–è¯„è®ºæ•°
                count_match = re.search(r'(?:å›ç­”æ•°|å›å¤æ•°)[ï¼š:](\d+)', line)
                comment_count = int(count_match.group(1)) if count_match else None

                return title, comment_count

        return None, None

    def _deduplicate_urls(self, url_list: List[Dict], id_key: str) -> List[Dict]:
        """æ ¹æ®IDå»é‡"""
        seen_ids = set()
        unique_urls = []

        for url_info in url_list:
            url_id = url_info.get(id_key)
            if url_id and url_id not in seen_ids:
                seen_ids.add(url_id)
                unique_urls.append(url_info)

        return unique_urls

    def save_results(self, urls: Dict[str, List[Dict]], output_file: str = "data/perplexity_urls.json"):
        """ä¿å­˜æœç´¢ç»“æœåˆ°JSONæ–‡ä»¶"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        result = {
            "search_date": datetime.now().isoformat(),
            "total_count": len(urls["zhihu"]) + len(urls["v2ex"]),
            "zhihu_count": len(urls["zhihu"]),
            "v2ex_count": len(urls["v2ex"]),
            "urls": urls
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    def display_results(self, urls: Dict[str, List[Dict]]):
        """æ˜¾ç¤ºæœç´¢ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ¯ æœç´¢ç»“æœæ±‡æ€»")
        print("="*60)

        if urls["zhihu"]:
            print(f"\nğŸ“š çŸ¥ä¹ ({len(urls['zhihu'])}ä¸ª):")
            for i, url_info in enumerate(urls["zhihu"], 1):
                title = url_info.get('title', 'æœªçŸ¥æ ‡é¢˜')
                count = url_info.get('estimated_comments', '?')
                print(f"  {i}. {url_info['url']}")
                print(f"     æ ‡é¢˜: {title}")
                print(f"     é¢„ä¼°å›ç­”æ•°: {count}")

        if urls["v2ex"]:
            print(f"\nğŸ’¬ V2EX ({len(urls['v2ex'])}ä¸ª):")
            for i, url_info in enumerate(urls["v2ex"], 1):
                title = url_info.get('title', 'æœªçŸ¥æ ‡é¢˜')
                count = url_info.get('estimated_comments', '?')
                print(f"  {i}. {url_info['url']}")
                print(f"     æ ‡é¢˜: {title}")
                print(f"     é¢„ä¼°å›å¤æ•°: {count}")

        print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨Perplexity APIæœç´¢ç¬¦åˆè¦æ±‚çš„è®¨è®ºURL"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        help="Perplexity API Keyï¼ˆä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡PERPLEXITY_API_KEYè®¾ç½®ï¼‰"
    )
    parser.add_argument(
        "--count",
        type=int,
        default=20,
        help="ç›®æ ‡URLæ•°é‡ï¼ˆé»˜è®¤: 20ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data/perplexity_urls.json",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/perplexity_urls.jsonï¼‰"
    )

    args = parser.parse_args()

    # è·å–API Key
    api_key = args.api_key or os.getenv("PERPLEXITY_API_KEY")

    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°API Keyï¼")
        print("\nè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æä¾›API Key:")
        print("  1. è®¾ç½®ç¯å¢ƒå˜é‡: export PERPLEXITY_API_KEY='your-api-key'")
        print("  2. ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: --api-key 'your-api-key'")
        sys.exit(1)

    print("ğŸš€ Perplexity URL æœç´¢å·¥å…·")
    print("="*60)
    print(f"ç›®æ ‡æ•°é‡: {args.count}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print("="*60)

    try:
        # åˆ›å»ºæœç´¢å™¨
        searcher = PerplexityURLSearcher(api_key)

        # æ‰§è¡Œæœç´¢
        urls = searcher.search_urls(target_count=args.count)

        # æ˜¾ç¤ºç»“æœ
        searcher.display_results(urls)

        # ä¿å­˜ç»“æœ
        searcher.save_results(urls, args.output)

        print("\nâœ… æœç´¢å®Œæˆï¼")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"  1. æŸ¥çœ‹æœç´¢ç»“æœ: cat {args.output}")
        print(f"  2. éªŒè¯URLçš„çœŸå®è¯„è®ºæ•°ï¼ˆå¯èƒ½éœ€è¦ï¼‰")
        print(f"  3. å°†éªŒè¯åçš„URLæ·»åŠ åˆ° config/target_urls.json")

    except Exception as e:
        print(f"\nâŒ æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
