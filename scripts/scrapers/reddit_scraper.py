"""
Redditæ•°æ®é‡‡é›†è„šæœ¬ - ä½¿ç”¨å…¬å¼€JSONæ¥å£

ä½¿ç”¨Redditçš„å…¬å¼€JSON APIé‡‡é›†AIå¯¹ç¨‹åºå‘˜å°±ä¸šå½±å“çš„è®¨è®º
æ— éœ€APIè®¤è¯ï¼Œå®Œå…¨å…è´¹ï¼

ä½¿ç”¨æ–¹æ³•:
ç›´æ¥è¿è¡Œ: pixi run python reddit_scraper.py
"""

import requests
import time
import json
from typing import List, Dict, Optional
from datetime import datetime
from pathlib import Path
from loguru import logger
from urllib.parse import quote


class RedditJSONScraper:
    """Redditå…¬å¼€JSONæ¥å£é‡‡é›†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.base_delay = 2  # æ¯ä¸ªè¯·æ±‚é—´éš”2ç§’ï¼Œé¿å…è¢«é™åˆ¶
        logger.info("âœ… Reddit JSONé‡‡é›†å™¨åˆå§‹åŒ–æˆåŠŸ")

    def search_subreddit(
        self,
        subreddit: str,
        query: str,
        min_comments: int = 100,
        limit: int = 100
    ) -> List[Dict]:
        """
        åœ¨æŒ‡å®šsubredditæœç´¢å¸–å­

        Args:
            subreddit: subredditåç§°ï¼ˆå¦‚"programming"ï¼‰
            query: æœç´¢å…³é”®è¯
            min_comments: æœ€å°‘è¯„è®ºæ•°
            limit: æœç´¢ç»“æœæ•°é‡é™åˆ¶

        Returns:
            ç¬¦åˆæ¡ä»¶çš„å¸–å­åˆ—è¡¨ï¼ˆä»…åŸºæœ¬ä¿¡æ¯ï¼Œä¸å«è¯„è®ºï¼‰
        """
        url = f"https://www.reddit.com/r/{subreddit}/search.json"
        params = {
            'q': query,
            'restrict_sr': 'on',  # é™åˆ¶åœ¨å½“å‰subreddit
            'sort': 'comments',   # æŒ‰è¯„è®ºæ•°æ’åº
            't': 'all',           # æ‰€æœ‰æ—¶é—´
            'limit': limit
        }

        try:
            logger.info(f"ğŸ” æœç´¢ r/{subreddit} - å…³é”®è¯: {query}")
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()
            posts = []

            for child in data['data']['children']:
                post = child['data']

                # è¿‡æ»¤ï¼šè¯„è®ºæ•°è¦å¤Ÿ
                if post['num_comments'] < min_comments:
                    continue

                # è¿‡æ»¤ï¼šä¸è¦è¢«åˆ é™¤çš„å¸–å­
                if post.get('removed_by_category') or post.get('removed'):
                    continue

                # ä¿å­˜åŸºæœ¬ä¿¡æ¯
                posts.append({
                    'id': post['id'],
                    'subreddit': post['subreddit'],
                    'title': post['title'],
                    'selftext': post.get('selftext', ''),
                    'author': post.get('author', '[deleted]'),
                    'created_utc': post['created_utc'],
                    'score': post['score'],
                    'upvote_ratio': post.get('upvote_ratio', 0),
                    'num_comments': post['num_comments'],
                    'permalink': post['permalink'],
                    'url': f"https://reddit.com{post['permalink']}"
                })

            logger.info(f"  âœ… æ‰¾åˆ° {len(posts)} ä¸ªç¬¦åˆæ¡ä»¶çš„å¸–å­")
            time.sleep(self.base_delay)  # å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            return posts

        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def get_post_with_comments(self, subreddit: str, post_id: str, max_comments: int = 100) -> Optional[Dict]:
        """
        è·å–å¸–å­è¯¦æƒ…å’Œè¯„è®º

        Args:
            subreddit: subredditåç§°
            post_id: å¸–å­ID
            max_comments: æœ€å¤šæå–è¯„è®ºæ•°

        Returns:
            å®Œæ•´çš„å¸–å­æ•°æ®ï¼ˆåŒ…å«è¯„è®ºï¼‰
        """
        url = f"https://www.reddit.com/r/{subreddit}/comments/{post_id}/.json"

        try:
            logger.info(f"  ğŸ“¥ è·å–å¸–å­ {post_id} çš„è¯„è®º...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            data = response.json()

            # ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¸–å­ä¿¡æ¯
            post_data = data[0]['data']['children'][0]['data']

            # ç¬¬äºŒä¸ªå…ƒç´ æ˜¯è¯„è®ºåˆ—è¡¨
            comments_data = data[1]['data']['children']

            # æå–å¸–å­åŸºæœ¬ä¿¡æ¯
            post = {
                'platform': 'reddit',
                'type': 'submission',
                'url': f"https://reddit.com{post_data['permalink']}",
                'subreddit': post_data['subreddit'],
                'scraped_at': datetime.now().isoformat(),
                'title': post_data['title'],
                'content': post_data.get('selftext', '[é“¾æ¥å¸–å­]'),
                'author': post_data.get('author', '[deleted]'),
                'created_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                'upvotes': post_data['score'],
                'upvote_ratio': post_data.get('upvote_ratio', 0),
                'comment_count': post_data['num_comments'],
                'comments': []
            }

            # æå–è¯„è®º
            comment_count = 0
            for comment_obj in comments_data:
                if comment_count >= max_comments:
                    break

                # è·³è¿‡"æ›´å¤šè¯„è®º"å ä½ç¬¦
                if comment_obj['kind'] == 'more':
                    continue

                comment = comment_obj['data']

                # è·³è¿‡è¢«åˆ é™¤çš„è¯„è®º
                if comment.get('author') == '[deleted]' and comment.get('body') == '[deleted]':
                    continue

                post['comments'].append({
                    'author': comment.get('author', '[deleted]'),
                    'content': comment.get('body', ''),
                    'created_at': datetime.fromtimestamp(comment['created_utc']).isoformat(),
                    'upvotes': comment['score'],
                })

                comment_count += 1

            logger.info(f"    âœ… æå–äº† {comment_count} æ¡è¯„è®º")

            # æ·»åŠ ç›¸å…³æ€§æ ‡è®°
            post['is_relevant'] = True
            post['relevance_note'] = 'Redditå…¬å¼€APIé‡‡é›† - AIå¯¹ç¨‹åºå‘˜å½±å“ç›¸å…³è®¨è®º'

            time.sleep(self.base_delay)  # å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            return post

        except Exception as e:
            logger.error(f"âŒ è·å–å¸–å­å¤±è´¥: {e}")
            return None

    def collect_posts(
        self,
        subreddits: List[str],
        keywords: List[str],
        min_comments: int = 100,
        target_count: int = 10
    ) -> List[Dict]:
        """
        é‡‡é›†Redditå¸–å­

        Args:
            subreddits: è¦æœç´¢çš„subredditåˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            min_comments: æœ€å°‘è¯„è®ºæ•°
            target_count: ç›®æ ‡é‡‡é›†æ•°é‡

        Returns:
            å®Œæ•´çš„å¸–å­åˆ—è¡¨ï¼ˆåŒ…å«è¯„è®ºï¼‰
        """
        all_posts = []
        collected_ids = set()

        for subreddit in subreddits:
            if len(all_posts) >= target_count:
                break

            for keyword in keywords:
                if len(all_posts) >= target_count:
                    break

                # æœç´¢å¸–å­ï¼ˆä»…åŸºæœ¬ä¿¡æ¯ï¼‰
                basic_posts = self.search_subreddit(
                    subreddit=subreddit,
                    query=keyword,
                    min_comments=min_comments,
                    limit=50
                )

                # è·å–æ¯ä¸ªå¸–å­çš„å®Œæ•´æ•°æ®ï¼ˆåŒ…æ‹¬è¯„è®ºï¼‰
                for basic_post in basic_posts:
                    if len(all_posts) >= target_count:
                        break

                    # å»é‡
                    if basic_post['id'] in collected_ids:
                        continue

                    # è·å–å®Œæ•´æ•°æ®
                    full_post = self.get_post_with_comments(
                        subreddit=basic_post['subreddit'],
                        post_id=basic_post['id'],
                        max_comments=100
                    )

                    if full_post:
                        all_posts.append(full_post)
                        collected_ids.add(basic_post['id'])
                        logger.info(f"âœ… å·²é‡‡é›† {len(all_posts)}/{target_count} ä¸ªå¸–å­")

        return all_posts

    def save_posts(self, posts: List[Dict], output_file: str):
        """
        ä¿å­˜å¸–å­æ•°æ®åˆ°JSONæ–‡ä»¶

        Args:
            posts: å¸–å­åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        logger.info(f"ğŸ“Š å…±ä¿å­˜ {len(posts)} ä¸ªå¸–å­")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹Redditæ•°æ®é‡‡é›†...")

    # åˆå§‹åŒ–é‡‡é›†å™¨
    scraper = RedditJSONScraper()

    # ç›®æ ‡subreddit
    subreddits = [
        'programming',           # ç¼–ç¨‹è®¨è®º
        'cscareerquestions',     # CSèŒä¸šé—®é¢˜
        'artificial',            # AIè®¨è®º
        'MachineLearning',       # æœºå™¨å­¦ä¹ 
        'learnprogramming',      # ç¼–ç¨‹å­¦ä¹ 
        'Python',                # Pythonç›¸å…³
        'javascript',            # JavaScriptç›¸å…³
    ]

    # æœç´¢å…³é”®è¯
    keywords = [
        'ChatGPT programmer job',
        'AI replace developers',
        'GPT impact software engineer',
        'artificial intelligence programming career',
        'ChatGPT coding job market',
    ]

    # é‡‡é›†å¸–å­
    posts = scraper.collect_posts(
        subreddits=subreddits,
        keywords=keywords,
        min_comments=100,  # æœ€å°‘100æ¡è¯„è®º
        target_count=15    # ç›®æ ‡15ä¸ªå¸–å­ï¼ˆç•™ç‚¹ä½™é‡ï¼‰
    )

    logger.info(f"ğŸ“Š é‡‡é›†å®Œæˆï¼å…±è·å– {len(posts)} ä¸ªå¸–å­")

    # ä¿å­˜ç»“æœ
    if posts:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/raw/reddit_posts_{timestamp}.json"
        scraper.save_posts(posts, output_file)

        # ç»Ÿè®¡ä¿¡æ¯
        total_comments = sum(len(post['comments']) for post in posts)
        avg_comments = total_comments / len(posts) if posts else 0

        logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  å¸–å­æ•°é‡: {len(posts)}")
        logger.info(f"  æ€»è¯„è®ºæ•°: {total_comments}")
        logger.info(f"  å¹³å‡æ¯å¸–è¯„è®ºæ•°: {avg_comments:.1f}")

        # æ˜¾ç¤ºsubredditåˆ†å¸ƒ
        subreddit_counts = {}
        for post in posts:
            sr = post['subreddit']
            subreddit_counts[sr] = subreddit_counts.get(sr, 0) + 1

        logger.info("  Subredditåˆ†å¸ƒ:")
        for sr, count in sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"    r/{sr}: {count} ä¸ªå¸–å­")

    else:
        logger.warning("âš ï¸ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ•°æ®")

    logger.info("âœ… Redditæ•°æ®é‡‡é›†å®Œæˆï¼")


if __name__ == "__main__":
    main()
