"""
æ•°æ®åˆå¹¶ä¸æ¸…æ´—è„šæœ¬ v2
åŠŸèƒ½ï¼šåˆå¹¶Redditå’ŒV2EXæ•°æ®ï¼Œè¿›è¡Œæ¸…æ´—å’Œæ ‡å‡†åŒ–å¤„ç†
ä½¿ç”¨æ›´å¥å£®çš„JSONè§£ææ–¹å¼
"""

import json
import os
import re
from datetime import datetime, timedelta
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(r"C:\Users\johntimeson\Desktop\ai_impact_data_collection")
DATA_RAW_DIR = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)


def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
    if not text:
        return ""

    # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)

    # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

    # å»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()

    return text


def normalize_date(date_str: str) -> str:
    """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD"""
    if not date_str:
        return ""

    # å¤„ç†ç›¸å¯¹æ—¶é—´ï¼ˆå¦‚ "7å¤©å‰"ï¼‰
    if "å¤©å‰" in str(date_str):
        try:
            days = int(re.search(r'(\d+)', str(date_str)).group(1))
            date = datetime.now() - timedelta(days=days)
            return date.strftime("%Y-%m-%d")
        except:
            pass

    # å¤„ç† "days ago" æ ¼å¼
    if "days ago" in str(date_str).lower() or "day ago" in str(date_str).lower():
        try:
            days = int(re.search(r'(\d+)', str(date_str)).group(1))
            date = datetime.now() - timedelta(days=days)
            return date.strftime("%Y-%m-%d")
        except:
            pass

    # å°è¯•è§£ææ ‡å‡†æ—¥æœŸæ ¼å¼
    date_formats = [
        "%Y-%m-%d",
        "%Y-%m-%dT%H:%M:%S.%fZ",
        "%Y-%m-%dT%H:%M:%S.%f",
        "%Y-%m-%d %H:%M:%S %z",
        "%Y-%m-%d %H:%M:%S",
    ]

    for fmt in date_formats:
        try:
            dt = datetime.strptime(str(date_str)[:26], fmt)
            return dt.strftime("%Y-%m-%d")
        except:
            continue

    # å°è¯•æå–æ—¥æœŸéƒ¨åˆ†
    match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})', str(date_str))
    if match:
        return match.group(1).replace('/', '-')

    return str(date_str)[:10] if date_str else ""


def safe_load_json(file_path: Path) -> dict | list | None:
    """å®‰å…¨åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # å°è¯•ä¿®å¤å¸¸è§JSONé—®é¢˜
            # æ›¿æ¢å¯èƒ½å¯¼è‡´é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
            content = content.replace('\r\n', '\n')
            return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"  JSONè§£æé”™è¯¯: {e}")
        # å°è¯•æ›´å®½æ¾çš„è§£æ
        try:
            import ast
            with open(file_path, 'r', encoding='utf-8') as f:
                return ast.literal_eval(f.read())
        except:
            pass
    except Exception as e:
        print(f"  è¯»å–é”™è¯¯: {e}")
    return None


def load_reddit_posts() -> list:
    """åŠ è½½Redditå¸–å­æ•°æ®"""
    posts = []

    for i in range(1, 11):
        file_path = DATA_RAW_DIR / f"reddit_post_{i}.json"
        if file_path.exists():
            print(f"  å¤„ç† reddit_post_{i}.json...")
            post = safe_load_json(file_path)
            if post:
                posts.append(post)
                print(f"    âœ“ æˆåŠŸ: {post.get('title', 'N/A')[:50]}...")
            else:
                print(f"    âœ— å¤±è´¥")

    return posts


def load_v2ex_posts() -> list:
    """åŠ è½½V2EXå¸–å­æ•°æ®"""
    file_path = DATA_RAW_DIR / "v2ex_ai_impact_posts.json"

    if file_path.exists():
        posts = safe_load_json(file_path)
        if posts:
            print(f"  âœ“ V2EXå¸–å­: {len(posts)} ä¸ª")
            return posts
        else:
            print(f"  âœ— V2EXåŠ è½½å¤±è´¥")

    return []


def standardize_reddit_post(post: dict) -> dict:
    """æ ‡å‡†åŒ–Redditå¸–å­æ ¼å¼"""
    comments = post.get('comments', [])

    standardized_comments = []
    for c in comments:
        if isinstance(c, dict):
            standardized_comments.append({
                "author": str(c.get('author', 'unknown')),
                "content": clean_text(str(c.get('content', ''))),
                "upvotes": int(c.get('upvotes', 0)) if c.get('upvotes') else 0,
                "created_at": normalize_date(c.get('created_at', '')),
            })

    url = post.get('url', '')
    post_id = url.split('/')[-2] if url and '/' in url else 'unknown'

    return {
        "id": f"reddit_{post_id}",
        "platform": "reddit",
        "subreddit": post.get('subreddit', ''),
        "url": url,
        "title": clean_text(str(post.get('title', ''))),
        "content": clean_text(str(post.get('content', ''))),
        "author": str(post.get('author', 'unknown')),
        "created_at": normalize_date(post.get('created_at', '')),
        "upvotes": int(post.get('upvotes', 0)) if post.get('upvotes') else 0,
        "comment_count": int(post.get('comment_count', len(comments))),
        "comments": standardized_comments,
        "language": "en",
        "scraped_at": str(post.get('scraped_at', '')),
    }


def standardize_v2ex_post(post: dict) -> dict:
    """æ ‡å‡†åŒ–V2EXå¸–å­æ ¼å¼"""
    comments = post.get('comments', [])

    standardized_comments = []
    for c in comments:
        if isinstance(c, dict):
            upvotes = c.get('upvotes', 0)
            if not isinstance(upvotes, int):
                upvotes = 0
            standardized_comments.append({
                "author": str(c.get('author', 'unknown')),
                "content": clean_text(str(c.get('content', ''))),
                "upvotes": upvotes,
                "created_at": normalize_date(c.get('created_at', c.get('time', ''))),
                "floor": c.get('floor', 0),
            })

    view_count = post.get('view_count', 0)
    if isinstance(view_count, str):
        # å¤„ç† "7480" æˆ– "7480 æ¬¡ç‚¹å‡»" æ ¼å¼
        match = re.search(r'(\d+)', view_count)
        view_count = int(match.group(1)) if match else 0

    return {
        "id": f"v2ex_{post.get('topic_id', 'unknown')}",
        "platform": "v2ex",
        "url": post.get('url', ''),
        "title": clean_text(str(post.get('title', ''))),
        "content": clean_text(str(post.get('content', ''))),
        "author": str(post.get('author', 'unknown')),
        "created_at": normalize_date(post.get('created_at', '')),
        "view_count": view_count,
        "comment_count": int(post.get('comment_count', len(comments))),
        "comments": standardized_comments,
        "tags": post.get('tags', []),
        "language": "zh",
        "scraped_at": str(post.get('scraped_at', '')),
    }


def extract_all_comments(posts: list) -> list:
    """æå–æ‰€æœ‰è¯„è®ºä¸ºç‹¬ç«‹åˆ—è¡¨"""
    all_comments = []

    for post in posts:
        post_id = post.get('id', '')
        platform = post.get('platform', '')
        post_title = post.get('title', '')
        post_date = post.get('created_at', '')

        for idx, comment in enumerate(post.get('comments', [])):
            all_comments.append({
                "comment_id": f"{post_id}_c{idx}",
                "post_id": post_id,
                "platform": platform,
                "post_title": post_title[:80] if post_title else '',
                "post_date": post_date,
                "author": comment.get('author', ''),
                "content": comment.get('content', ''),
                "upvotes": comment.get('upvotes', 0),
                "created_at": comment.get('created_at', ''),
                "language": post.get('language', ''),
            })

    return all_comments


def generate_statistics(posts: list, comments: list) -> dict:
    """ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    reddit_posts = [p for p in posts if p['platform'] == 'reddit']
    v2ex_posts = [p for p in posts if p['platform'] == 'v2ex']

    reddit_comments = [c for c in comments if c['platform'] == 'reddit']
    v2ex_comments = [c for c in comments if c['platform'] == 'v2ex']

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    year_dist = {}
    for p in posts:
        year = str(p.get('created_at', ''))[:4]
        if year and year.isdigit():
            year_dist[year] = year_dist.get(year, 0) + 1

    # æŒ‰å¹³å°ç»Ÿè®¡è¯„è®ºæ•°
    platform_comment_stats = {}
    for p in posts:
        platform = p['platform']
        if platform not in platform_comment_stats:
            platform_comment_stats[platform] = []
        platform_comment_stats[platform].append(p['comment_count'])

    stats = {
        "generated_at": datetime.now().isoformat(),
        "data_summary": {
            "total_posts": len(posts),
            "total_comments": len(comments),
            "avg_comments_per_post": round(len(comments) / len(posts), 1) if posts else 0,
        },
        "platform_distribution": {
            "reddit": {
                "posts": len(reddit_posts),
                "comments": len(reddit_comments),
                "avg_comments": round(len(reddit_comments) / len(reddit_posts), 1) if reddit_posts else 0,
            },
            "v2ex": {
                "posts": len(v2ex_posts),
                "comments": len(v2ex_comments),
                "avg_comments": round(len(v2ex_comments) / len(v2ex_posts), 1) if v2ex_posts else 0,
            }
        },
        "language_distribution": {
            "english": len(reddit_comments),
            "chinese": len(v2ex_comments),
        },
        "year_distribution": dict(sorted(year_dist.items())),
        "time_range": {
            "earliest": min([p.get('created_at', '9999') for p in posts if p.get('created_at')]),
            "latest": max([p.get('created_at', '0000') for p in posts if p.get('created_at')]),
        }
    }

    return stats


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ•°æ®åˆå¹¶ä¸æ¸…æ´— v2")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    print("\n[1/4] åŠ è½½åŸå§‹æ•°æ®...")
    reddit_posts = load_reddit_posts()
    v2ex_posts = load_v2ex_posts()

    print(f"\n  åŠ è½½ç»“æœ: Reddit {len(reddit_posts)} ä¸ª, V2EX {len(v2ex_posts)} ä¸ª")

    # 2. æ ‡å‡†åŒ–æ•°æ®
    print("\n[2/4] æ ‡å‡†åŒ–æ•°æ®æ ¼å¼...")
    all_posts = []

    for post in reddit_posts:
        try:
            standardized = standardize_reddit_post(post)
            all_posts.append(standardized)
        except Exception as e:
            print(f"  âœ— Redditå¸–å­æ ‡å‡†åŒ–å¤±è´¥: {e}")

    for post in v2ex_posts:
        try:
            standardized = standardize_v2ex_post(post)
            all_posts.append(standardized)
        except Exception as e:
            print(f"  âœ— V2EXå¸–å­æ ‡å‡†åŒ–å¤±è´¥: {e}")

    print(f"  âœ“ æ ‡å‡†åŒ–å®Œæˆ: {len(all_posts)} ä¸ªå¸–å­")

    # 3. æå–è¯„è®º
    print("\n[3/4] æå–è¯„è®ºæ•°æ®...")
    all_comments = extract_all_comments(all_posts)
    print(f"  âœ“ æå–å®Œæˆ: {len(all_comments)} æ¡è¯„è®º")

    # 4. ç”Ÿæˆç»Ÿè®¡
    print("\n[4/4] ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    stats = generate_statistics(all_posts, all_comments)

    # ä¿å­˜ç»“æœ
    print("\nä¿å­˜å¤„ç†åçš„æ•°æ®...")

    # ä¿å­˜åˆå¹¶åçš„å¸–å­æ•°æ®
    posts_output = DATA_PROCESSED_DIR / "merged_posts.json"
    with open(posts_output, 'w', encoding='utf-8') as f:
        json.dump(all_posts, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ å¸–å­æ•°æ®: {posts_output}")

    # ä¿å­˜è¯„è®ºæ•°æ®
    comments_output = DATA_PROCESSED_DIR / "all_comments.json"
    with open(comments_output, 'w', encoding='utf-8') as f:
        json.dump(all_comments, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ è¯„è®ºæ•°æ®: {comments_output}")

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_output = DATA_PROCESSED_DIR / "data_statistics.json"
    with open(stats_output, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ ç»Ÿè®¡ä¿¡æ¯: {stats_output}")

    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦")
    print("=" * 60)

    summary = stats['data_summary']
    print(f"\nğŸ“Œ æ€»ä½“æ•°æ®:")
    print(f"   - æ€»å¸–å­æ•°: {summary['total_posts']}")
    print(f"   - æ€»è¯„è®ºæ•°: {summary['total_comments']}")
    print(f"   - å¹³å‡æ¯å¸–è¯„è®ºæ•°: {summary['avg_comments_per_post']}")

    print(f"\nğŸ“Œ å¹³å°åˆ†å¸ƒ:")
    for platform, data in stats['platform_distribution'].items():
        print(f"   - {platform.upper()}: {data['posts']} å¸–å­, {data['comments']} è¯„è®º (å¹³å‡ {data['avg_comments']})")

    print(f"\nğŸ“Œ è¯­è¨€åˆ†å¸ƒ:")
    lang = stats['language_distribution']
    print(f"   - è‹±æ–‡: {lang['english']} æ¡")
    print(f"   - ä¸­æ–‡: {lang['chinese']} æ¡")

    print(f"\nğŸ“Œ æ—¶é—´èŒƒå›´:")
    time_range = stats['time_range']
    print(f"   - æœ€æ—©: {time_range['earliest']}")
    print(f"   - æœ€æ™š: {time_range['latest']}")

    print(f"\nğŸ“Œ å¹´ä»½åˆ†å¸ƒ:")
    for year, count in stats['year_distribution'].items():
        print(f"   - {year}: {count} å¸–å­")

    print("\n" + "=" * 60)
    print("âœ… æ•°æ®åˆå¹¶ä¸æ¸…æ´—å®Œæˆ!")
    print("=" * 60)

    return all_posts, all_comments, stats


if __name__ == "__main__":
    main()
