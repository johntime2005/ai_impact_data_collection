#!/usr/bin/env python3
"""æ•°æ®åˆå¹¶ä¸æ¸…æ´—è„šæœ¬ v3 - ä½¿ç”¨ä¿®å¤åçš„Redditæ•°æ®"""

import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict

def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
    if not text:
        return ""

    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
    return text.strip()


def parse_date(date_str: str) -> str:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºæ ‡å‡†æ ¼å¼"""
    if not date_str:
        return ""

    # å¤„ç†å„ç§æ—¥æœŸæ ¼å¼
    formats = [
        "%Y-%m-%dT%H:%M:%S.%fZ",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
        "%Y/%m/%d",
    ]

    for fmt in formats:
        try:
            dt = datetime.strptime(date_str, fmt)
            return dt.strftime("%Y-%m-%d")
        except ValueError:
            continue

    # å°è¯•æå–å¹´æœˆæ—¥
    match = re.search(r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})', date_str)
    if match:
        return f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"

    return date_str


def standardize_reddit_post(post: dict) -> dict:
    """æ ‡å‡†åŒ–Redditå¸–å­æ ¼å¼"""
    comments = []
    for c in post.get('comments', []):
        comments.append({
            'author': c.get('author', ''),
            'content': clean_text(c.get('content', '')),
            'upvotes': c.get('upvotes', 0),
            'created_at': parse_date(c.get('created_at', '')),
            'platform': 'reddit'
        })

    return {
        'id': f"reddit_{hash(post.get('url', '')) % 100000}",
        'platform': 'reddit',
        'title': clean_text(post.get('title', '')),
        'content': clean_text(post.get('content', '')),
        'author': post.get('author', ''),
        'url': post.get('url', ''),
        'created_at': parse_date(post.get('created_at', '')),
        'subreddit': post.get('subreddit', ''),
        'upvotes': post.get('upvotes', 0),
        'comment_count': len(comments),
        'comments': comments,
        'language': 'en'
    }


def standardize_v2ex_post(post: dict) -> dict:
    """æ ‡å‡†åŒ–V2EXå¸–å­æ ¼å¼"""
    comments = []
    for c in post.get('comments', []):
        comments.append({
            'author': c.get('author', ''),
            'content': clean_text(c.get('content', '')),
            'upvotes': c.get('upvotes', 0),
            'created_at': parse_date(c.get('created_at', '')),
            'platform': 'v2ex'
        })

    return {
        'id': f"v2ex_{post.get('id', hash(post.get('url', '')) % 100000)}",
        'platform': 'v2ex',
        'title': clean_text(post.get('title', '')),
        'content': clean_text(post.get('content', '')),
        'author': post.get('author', ''),
        'url': post.get('url', ''),
        'created_at': parse_date(post.get('created_at', '')),
        'node': post.get('node', ''),
        'upvotes': post.get('upvotes', 0),
        'comment_count': len(comments),
        'comments': comments,
        'language': 'zh'
    }


def main():
    raw_dir = Path("data/raw")
    processed_dir = Path("data/processed")
    processed_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("æ•°æ®åˆå¹¶ä¸æ¸…æ´— v3")
    print("=" * 60)

    all_posts = []

    # åŠ è½½ä¿®å¤åçš„Redditæ•°æ®
    print("\n[1/4] åŠ è½½Redditæ•°æ®...")
    reddit_fixed_path = raw_dir / "reddit_posts_fixed.json"
    if reddit_fixed_path.exists():
        with open(reddit_fixed_path, 'r', encoding='utf-8') as f:
            reddit_posts = json.load(f)
        print(f"  âœ“ åŠ è½½ {len(reddit_posts)} ä¸ªRedditå¸–å­")

        for post in reddit_posts:
            std_post = standardize_reddit_post(post)
            all_posts.append(std_post)

    # åŠ è½½V2EXæ•°æ®
    print("\n[2/4] åŠ è½½V2EXæ•°æ®...")
    v2ex_path = raw_dir / "v2ex_ai_impact_posts.json"
    if v2ex_path.exists():
        with open(v2ex_path, 'r', encoding='utf-8') as f:
            v2ex_posts = json.load(f)
        print(f"  âœ“ åŠ è½½ {len(v2ex_posts)} ä¸ªV2EXå¸–å­")

        for post in v2ex_posts:
            std_post = standardize_v2ex_post(post)
            all_posts.append(std_post)

    print(f"\n  æ€»è®¡: {len(all_posts)} ä¸ªå¸–å­")

    # æå–æ‰€æœ‰è¯„è®º
    print("\n[3/4] æå–è¯„è®ºæ•°æ®...")
    all_comments = []
    for post in all_posts:
        for comment in post.get('comments', []):
            comment['post_id'] = post['id']
            comment['post_title'] = post['title']
            all_comments.append(comment)

    print(f"  âœ“ æå–å®Œæˆ: {len(all_comments)} æ¡è¯„è®º")

    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    print("\n[4/4] ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")

    platform_stats = defaultdict(lambda: {'posts': 0, 'comments': 0})
    year_stats = defaultdict(int)
    language_stats = defaultdict(int)

    for post in all_posts:
        platform = post['platform']
        platform_stats[platform]['posts'] += 1
        platform_stats[platform]['comments'] += len(post.get('comments', []))

        # å¹´ä»½ç»Ÿè®¡
        date_str = post.get('created_at', '')
        if date_str:
            year = date_str[:4]
            year_stats[year] += 1

        # è¯­è¨€ç»Ÿè®¡
        for comment in post.get('comments', []):
            lang = post.get('language', 'unknown')
            language_stats[lang] += 1

    statistics = {
        'total_posts': len(all_posts),
        'total_comments': len(all_comments),
        'avg_comments_per_post': round(len(all_comments) / len(all_posts), 1) if all_posts else 0,
        'platform_distribution': dict(platform_stats),
        'year_distribution': dict(sorted(year_stats.items())),
        'language_distribution': dict(language_stats),
        'date_range': {
            'earliest': min((p.get('created_at', '9999') for p in all_posts), default=''),
            'latest': max((p.get('created_at', '') for p in all_posts), default='')
        }
    }

    # ä¿å­˜æ•°æ®
    print("\nä¿å­˜å¤„ç†åçš„æ•°æ®...")

    posts_path = processed_dir / "merged_posts.json"
    with open(posts_path, 'w', encoding='utf-8') as f:
        json.dump(all_posts, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ å¸–å­æ•°æ®: {posts_path}")

    comments_path = processed_dir / "all_comments.json"
    with open(comments_path, 'w', encoding='utf-8') as f:
        json.dump(all_comments, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ è¯„è®ºæ•°æ®: {comments_path}")

    stats_path = processed_dir / "data_statistics.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(statistics, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ ç»Ÿè®¡ä¿¡æ¯: {stats_path}")

    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦")
    print("=" * 60)

    print(f"\nğŸ“Œ æ€»ä½“æ•°æ®:")
    print(f"   - æ€»å¸–å­æ•°: {statistics['total_posts']}")
    print(f"   - æ€»è¯„è®ºæ•°: {statistics['total_comments']}")
    print(f"   - å¹³å‡æ¯å¸–è¯„è®ºæ•°: {statistics['avg_comments_per_post']}")

    print(f"\nğŸ“Œ å¹³å°åˆ†å¸ƒ:")
    for platform, stats in platform_stats.items():
        avg = round(stats['comments'] / stats['posts'], 1) if stats['posts'] > 0 else 0
        print(f"   - {platform.upper()}: {stats['posts']} å¸–å­, {stats['comments']} è¯„è®º (å¹³å‡ {avg})")

    print(f"\nğŸ“Œ è¯­è¨€åˆ†å¸ƒ:")
    for lang, count in language_stats.items():
        lang_name = "è‹±æ–‡" if lang == "en" else "ä¸­æ–‡" if lang == "zh" else lang
        print(f"   - {lang_name}: {count} æ¡")

    print(f"\nğŸ“Œ æ—¶é—´èŒƒå›´:")
    print(f"   - æœ€æ—©: {statistics['date_range']['earliest']}")
    print(f"   - æœ€æ™š: {statistics['date_range']['latest']}")

    print(f"\nğŸ“Œ å¹´ä»½åˆ†å¸ƒ:")
    for year, count in sorted(year_stats.items()):
        print(f"   - {year}: {count} å¸–å­")

    print("\n" + "=" * 60)
    print("âœ… æ•°æ®åˆå¹¶ä¸æ¸…æ´—å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
